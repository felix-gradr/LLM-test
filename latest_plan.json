['Here are three immediate, high-leverage tasks you can pick up next. Each is small in scope but moves us sharply toward “never getting stuck” and “LLM-driven self-improvement.”', 'Build the orchestration entry-point (root.py)', '• Create a new root.py at the repo root with a `main()` that:', '– Detects and reads `seed.txt` if present, enqueues its contents as initial tasks, then deletes it', '– Wraps a call to `fallback.agent_step()` in a try/except so that any crash is caught and rerouted back into `fallback.agent_step()` as a rescue', '– Always writes out a brief run-summary (e.g. timestamp + status) so we can confirm “something always happened.”', 'Outcome: “python -m root” never crashes and always invokes a fallback pass', 'Finish out `fallback.agent_step()`’s LLM round-trip and file application', '• After you’ve constructed `SYSTEM_PROMPT_WITH_GOAL` and `user_prompt`, actually invoke the AzureOpenAI client with a chat completion call', '• Agree on a simple JSON schema for the model to return a list of edits, e.g.:', '{ “edits”: [ { “path”: “coder.py”, “patch”: “<unified-diff>” }, … ] }', '• Parse that JSON from the LLM reply, and for each edit apply the patch to disk', 'Outcome: the fallback agent will turn LLM suggestions into real file changes', 'Add a small `patch_utils.py` (or extend fallback) with an `apply_patch()` helper', '• Implement a function that takes a unified-diff string (or full new file text) plus a path and safely writes/apply it', '• Include sanity checks (e.g., ensure the target file exists, back it up, validate hunks)', 'Outcome: you’ll have a reusable, tested utility so every agent-generated diff can be applied safely', 'Tackling these three will give you a robust “root → fallback → file edit” pipeline that never gets stuck, and lays the groundwork for further LLM-driven self-improvement']